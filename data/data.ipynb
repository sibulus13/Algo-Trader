{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pandas as pd\n",
    "import pandas_market_calendars as mcal\n",
    "import ta\n",
    "from alpaca_trade_api.rest import REST, TimeFrame\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler, maxabs_scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FMP_API_KEY = \"45d2c43abf8e1e4ba47884a88f378027\"\n",
    "POLYGON_KEY = \"AVs1KTDp0aDoioS_EQ2KN63JiIyTaKLf\"\n",
    "FINNHUB_KEY = \"ceivrvqad3if39n22g3gceivrvqad3if39n22g40\"\n",
    "TWELVEDATA_KEY = \"deb706049cdd47058e1210eacc3aefa5\"\n",
    "ALPHA_VANTAGO_KEY = \"PX771WNGSYWHEKLW\"\n",
    "NASDAQ_DATA_LINK_KEY = 'ouH2XbhJ1k9zz-6KtKPz'\n",
    "ALPACA_API_KEY='PKNHYYQ7OUDKJK3NP6EX'\n",
    "ALPACA_S_KEY='zo3qv6aUJJI14aSY2AgUOfuDufcapDx6yFBBixGD'\n",
    "ALPACA_BASE_URL = 'https://data.alpaca.markets/v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recorded start date: 2023-01-24\n",
      "2023-01-23\n",
      "recorded day before start date: 2023-01-23\n",
      "fetching new data\n",
      "(3250, 9)\n",
      "                            open   high     low   close  volume  trade_count  \\\n",
      "timestamp                                                                      \n",
      "2023-01-23 09:00:00+00:00  138.5  138.5  137.75  137.75    3970          148   \n",
      "\n",
      "                                 vwap                 timestamp      unixTime  \n",
      "timestamp                                                                      \n",
      "2023-01-23 09:00:00+00:00  137.942632 2023-01-23 09:00:00+00:00  1.674493e+09  \n",
      "                             open    high     low   close  volume  \\\n",
      "timestamp                                                           \n",
      "2023-01-27 00:59:00+00:00  143.67  143.69  143.66  143.69    7243   \n",
      "\n",
      "                           trade_count        vwap                 timestamp  \\\n",
      "timestamp                                                                      \n",
      "2023-01-27 00:59:00+00:00           46  143.678176 2023-01-27 00:59:00+00:00   \n",
      "\n",
      "                               unixTime  \n",
      "timestamp                                \n",
      "2023-01-27 00:59:00+00:00  1.674810e+09  \n",
      "cleaning raw data\n",
      "creating additional data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Michael\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ta\\trend.py:780: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  dip[idx] = 100 * (self._dip[idx] / value)\n",
      "c:\\Users\\Michael\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ta\\trend.py:785: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  din[idx] = 100 * (self._din[idx] / value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning up data\n",
      "saving to csv\n",
      "done\n",
      "2023-01-27 00:59:00+00:00\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = r'D:\\repo\\stonks\\data\\dataset'\n",
    "api = REST(ALPACA_API_KEY, ALPACA_S_KEY, ALPACA_BASE_URL, api_version='v2')\n",
    "\n",
    "\n",
    "def raw_csv_path(sym):\n",
    "    # return fr'D:\\repo\\stocks\\stonkAI\\data\\dataset\\{sym}.csv'\n",
    "    return os.path.join(DATASET_DIR, f'{sym}.csv')\n",
    "\n",
    "\n",
    "def data_csv_path(sym):\n",
    "    return os.path.join(DATASET_DIR, f'{sym}_data.csv')\n",
    "\n",
    "\n",
    "def file_path(sym, type, file_type='csv', dir=DATASET_DIR):\n",
    "    return os.path.join(dir, f'{sym}_{type}.{file_type}')\n",
    "\n",
    "\n",
    "def add_unix(df):\n",
    "    df['timestamp'] = df.index\n",
    "    df['unixTime'] = [time.mktime(ts.timetuple()) for ts in df['timestamp']]\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_last_available_day(timestamp_str: str):\n",
    "    '''fetches the date of the last available trading day'''\n",
    "    timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d')\n",
    "    day_before = timestamp - timedelta(days=1)\n",
    "    timestamp_str = day_before.strftime('%Y-%m-%d')\n",
    "    return timestamp_str\n",
    "\n",
    "\n",
    "def create_output(data, num_days=5):\n",
    "    # Create output columns for high and low range of 15 days\n",
    "    data_points_per_day = 447\n",
    "    indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=num_days *\n",
    "                                                        data_points_per_day)\n",
    "    data[f'highest_in_{num_days}_days'] = data['high'].rolling(\n",
    "        window=indexer, min_periods=1).max()\n",
    "    data[f'lowest_in_{num_days}_days'] = data['low'].rolling(\n",
    "        window=indexer, min_periods=1).min()\n",
    "    return data\n",
    "\n",
    "\n",
    "def fetch_new_data(sym, start_date, end_date=None):\n",
    "    '''\n",
    "    Grabs last recorded date from CSV and fetches new data from that date to today\n",
    "    Saves all datasets and latest scaled entry with corresponding timestamp\n",
    "    '''\n",
    "    tf = TimeFrame.Minute\n",
    "    adj = 'raw'\n",
    "    mode = 'w'\n",
    "    save = True\n",
    "    header = True\n",
    "    old_data = pd.DataFrame()\n",
    "    try:\n",
    "        old_data = pd.read_csv(raw_csv_path(sym))\n",
    "        # last_recorded_timestamp = old_data.iloc[-1]['timestamp'][:10]\n",
    "        start_date = old_data.iloc[-1]['timestamp'][:10]\n",
    "        start_date = get_last_available_day(start_date)\n",
    "        print(f'recorded day before start date: {start_date}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'ERROR: {e}')\n",
    "        print('no old data found')\n",
    "    # get new data here\n",
    "    # print('fetching new data')\n",
    "    data = api.get_bars(sym,\n",
    "                        tf,\n",
    "                        start=start_date,\n",
    "                        end=end_date,\n",
    "                        adjustment=adj).df\n",
    "    data = add_unix(data)\n",
    "    print(data.shape)\n",
    "    print(data.head(1))\n",
    "    print(data.tail(1))\n",
    "    if data.empty:\n",
    "        return\n",
    "    print('cleaning raw data')\n",
    "    all_data = pd.concat([data,\n",
    "                          old_data]).drop_duplicates().reset_index(drop=True)\n",
    "    all_data.sort_values(by=['unixTime'], inplace=True)\n",
    "    all_data = all_data[all_data.columns.drop(\n",
    "        list(all_data.filter(regex='Unnamed')))]\n",
    "    # TODO validate data sparsity\n",
    "\n",
    "    print('creating additional data')\n",
    "    # create outputs\n",
    "    all_data_with_outputs = create_output(all_data)\n",
    "\n",
    "    # grab technical indicators\n",
    "    all_data_with_ta_and_outputs = ta.add_all_ta_features(\n",
    "        all_data_with_outputs,\n",
    "        open=\"open\",\n",
    "        high=\"high\",\n",
    "        low=\"low\",\n",
    "        close=\"close\",\n",
    "        volume=\"volume\")\n",
    "\n",
    "    # all_data_with_ta_and_outputs\n",
    "    # remove first 50 rows to remove null values from technical indicators\n",
    "    all_data_with_ta_and_outputs = all_data_with_ta_and_outputs.iloc[\n",
    "        50:].reset_index(drop=True)\n",
    "    last_time_stamp = all_data_with_ta_and_outputs.iloc[-1]['timestamp']\n",
    "    last_unix = all_data_with_ta_and_outputs.iloc[-1]['unixTime']\n",
    "    # clean up data\n",
    "    print('cleaning up data')\n",
    "    # normalize outputs by dividing by close\n",
    "    all_data_with_ta_and_outputs[\n",
    "        'highest_in_5_days_percent'] = all_data_with_ta_and_outputs[\n",
    "            'highest_in_5_days'] / all_data_with_ta_and_outputs['close']\n",
    "    all_data_with_ta_and_outputs[\n",
    "        'lowest_in_5_days_percent'] = all_data_with_ta_and_outputs[\n",
    "            'lowest_in_5_days'] / all_data_with_ta_and_outputs['close']\n",
    "    # remove columns timestamp, unixTime, and the index\n",
    "    outputs = all_data_with_ta_and_outputs[[\n",
    "        'close', 'highest_in_5_days', 'highest_in_5_days_percent',\n",
    "        'lowest_in_5_days', 'lowest_in_5_days_percent'\n",
    "    ]]\n",
    "    inputs = all_data_with_ta_and_outputs.drop(columns=[\n",
    "        'timestamp', 'unixTime', 'highest_in_5_days', 'lowest_in_5_days',\n",
    "        'highest_in_5_days_percent', 'lowest_in_5_days_percent'\n",
    "    ])\n",
    "    inputs.dropna(inplace=True, axis=1)\n",
    "    print('inputs.shape', inputs.shape)\n",
    "    print(inputs.columns)\n",
    "    scaled_inputs = maxabs_scale.scale(inputs)\n",
    "    # save data to csv\n",
    "    if save:\n",
    "        print('saving to csv')\n",
    "        all_data.to_csv(raw_csv_path(sym), mode=mode, header=header)\n",
    "        all_data_with_ta_and_outputs.to_csv(data_csv_path(sym),\n",
    "                                            mode=mode,\n",
    "                                            header=header)\n",
    "        inputs.to_csv(file_path(sym, 'inputs_raw'), mode=mode, header=header)\n",
    "        scaled_inputs.to_csv(file_path(sym, 'inputs_scaled'),\n",
    "                             mode=mode,\n",
    "                             header=header)\n",
    "        outputs.to_csv(file_path(sym, 'outputs_raw'), mode=mode, header=header)\n",
    "        # savve last entry of scaled_inputs to csv\n",
    "        last_scaled_input = scaled_inputs.iloc[-1:]\n",
    "        last_scaled_input.to_csv(file_path(sym, 'last_input_entry', 'csv'),\n",
    "                                 mode=mode,\n",
    "                                 header=header)\n",
    "        with open(file_path(sym, 'last_unix', 'txt'), 'w') as f:\n",
    "            f.write(str(last_unix))\n",
    "    print('done')\n",
    "    print(last_time_stamp)\n",
    "    return last_time_stamp, last_scaled_input\n",
    "\n",
    "\n",
    "sym = 'AAPL'\n",
    "start_date = '2000-01-01'\n",
    "end_date = '2023-01-26'\n",
    "fetch_new_data(sym, start_date=start_date, end_date=end_date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
